{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332bd1f-8618-444f-bccd-e4e356d5bc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51f7ba5322b4f9aaa7bae9ce85c166c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c318ff76405a4a05bbf583640ad5cc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e64e7a2fb9f436b8818d8fdcb0109a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d97b58aec543be908fc18ce23b53b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Example: Streamed quality training on a Hugging Face dataset.\n",
    "\n",
    "This script demonstrates how to:\n",
    "- Load the Rapidata Imagen preference dataset in streaming mode\n",
    "- Derive quality quotients from human preference and alignment scores\n",
    "- Train a small Brain with stacked Wanderer plugins and neuroplasticity\n",
    "- Employ a custom SelfAttention routine combined with adaptive grad clipping\n",
    "- Gate Wanderer/neuroplasticity plugins via ``autoplugin`` and let neurons\n",
    "  switch types dynamically through ``autoneuron``\n",
    "\n",
    "Usage:\n",
    "    py -3 examples/run_hf_image_quality.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Iterator, Any, Dict, Optional\n",
    "import os\n",
    "import hashlib\n",
    "from collections import OrderedDict\n",
    "\n",
    "from marble.marblemain import (\n",
    "    Brain,\n",
    "    UniversalTensorCodec,\n",
    "    make_datapair,\n",
    "    run_training_with_datapairs,\n",
    "    load_hf_streaming_dataset,\n",
    "    SelfAttention,\n",
    ")\n",
    "import marble.plugins  # ensure plugin discovery\n",
    "from marble.plugins.selfattention_adaptive_grad_clip import AdaptiveGradClipRoutine\n",
    "from marble.plugins.selfattention_findbestneurontype import FindBestNeuronTypeRoutine\n",
    "from marble.plugins.selfattention_noise_profiler import ContextAwareNoiseRoutine\n",
    "\n",
    "\n",
    "class QualityAwareRoutine:\n",
    "    \"\"\"Adjust LR based on recent loss trend for stability.\"\"\"\n",
    "\n",
    "    def __init__(self, window: int = 8, decay: float = 0.9, grow: float = 1.1) -> None:\n",
    "        self.window = int(window)\n",
    "        self.decay = float(decay)\n",
    "        self.grow = float(grow)\n",
    "\n",
    "    def after_step(self, sa: SelfAttention, ro, wanderer, step_idx: int, ctx):\n",
    "        hist = sa.history(self.window)\n",
    "        if len(hist) < 2:\n",
    "            return None\n",
    "        losses = [h.get(\"current_loss\") for h in hist if isinstance(h.get(\"current_loss\"), (int, float))]\n",
    "        if len(losses) < 2:\n",
    "            return None\n",
    "        prev, cur = losses[-2], losses[-1]\n",
    "        base_lr = sa.get_param(\"lr_override\") or sa.get_param(\"current_lr\") or 1e-3\n",
    "        try:\n",
    "            base_lr = float(base_lr)\n",
    "        except Exception:\n",
    "            base_lr = 1e-3\n",
    "        if cur > prev:\n",
    "            new_lr = max(1e-5, base_lr * self.decay)\n",
    "        else:\n",
    "            new_lr = min(5e-3, base_lr * self.grow)\n",
    "        return {\"lr_override\": float(new_lr)}\n",
    "\n",
    "\n",
    "class _ImageEncodingLRUCache:\n",
    "    \"\"\"Simple LRU cache that stores the encoded representation of images.\n",
    "\n",
    "    - Keys are derived from common URL/path attributes or a stable repr digest.\n",
    "    - Values are the result of `UniversalTensorCodec.encode(image_obj)`.\n",
    "    - Used only within this example to avoid repeated downloads when the\n",
    "      dataset yields the same image multiple times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_items: int = 20, enabled: bool = True) -> None:\n",
    "        self.enabled = bool(enabled)\n",
    "        self.max_items = int(max_items)\n",
    "        self._od: \"OrderedDict[str, Any]\" = OrderedDict()\n",
    "\n",
    "    def _make_key(self, obj: Any) -> str:\n",
    "        # Prefer explicit URL/path attributes when present (to avoid fetching bytes)\n",
    "        try:\n",
    "            if isinstance(obj, dict):\n",
    "                for k in (\"url\", \"uri\", \"image_url\", \"path\", \"filename\"):\n",
    "                    if k in obj and isinstance(obj[k], str):\n",
    "                        return f\"url:{obj[k]}\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        for attr in (\"url\", \"uri\", \"path\", \"filename\"):\n",
    "            try:\n",
    "                v = getattr(obj, attr, None)\n",
    "                if isinstance(v, str) and v:\n",
    "                    return f\"url:{v}\"\n",
    "            except Exception:\n",
    "                pass\n",
    "        # If it looks like a plain string that could be a URL or path, use it\n",
    "        if isinstance(obj, str) and obj:\n",
    "            return f\"url:{obj}\"\n",
    "        # Fallback: stable digest of repr (avoids large materialization)\n",
    "        try:\n",
    "            r = repr(obj)\n",
    "        except Exception:\n",
    "            r = f\"{type(obj).__name__}:{id(obj)}\"\n",
    "        return f\"repr:{hashlib.sha256(r.encode('utf-8', errors='ignore')).hexdigest()}\"\n",
    "\n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        if not self.enabled:\n",
    "            return None\n",
    "        if key in self._od:\n",
    "            val = self._od.pop(key)\n",
    "            self._od[key] = val  # move to end (most recently used)\n",
    "            return val\n",
    "        return None\n",
    "\n",
    "    def put(self, key: str, value: Any) -> None:\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        if key in self._od:\n",
    "            self._od.pop(key)\n",
    "        self._od[key] = value\n",
    "        while len(self._od) > max(0, self.max_items):\n",
    "            self._od.popitem(last=False)\n",
    "\n",
    "    def get_or_encode(self, obj: Any, codec: UniversalTensorCodec):\n",
    "        key = self._make_key(obj)\n",
    "        cached = self.get(key)\n",
    "        if cached is not None:\n",
    "            return key, cached\n",
    "        enc = codec.encode(obj)\n",
    "        self.put(key, enc)\n",
    "        return key, enc\n",
    "\n",
    "\n",
    "def _sample_pairs(ds, codec: UniversalTensorCodec, cache: _ImageEncodingLRUCache) -> Iterator:\n",
    "    for ex in ds:\n",
    "        prompt = ex.get_raw(\"prompt\")\n",
    "        img1 = ex.get_raw(\"image1\")\n",
    "        img2 = ex.get_raw(\"image2\")\n",
    "        pref1 = float(ex.get_raw(\"weighted_results_image1_preference\"))\n",
    "        pref2 = float(ex.get_raw(\"weighted_results_image2_preference\"))\n",
    "        al1 = float(ex.get_raw(\"weighted_results_image1_alignment\"))\n",
    "        al2 = float(ex.get_raw(\"weighted_results_image2_alignment\"))\n",
    "        q1 = (pref1 + al1) / 2.0\n",
    "        q2 = (pref2 + al2) / 2.0\n",
    "        # Use cache to encode images once; store only the encoded form.\n",
    "        key1, _ = cache.get_or_encode(img1, codec)\n",
    "        key2, _ = cache.get_or_encode(img2, codec)\n",
    "        # Pass lightweight keys in the datapair to avoid triggering downloads on encode.\n",
    "        yield make_datapair({\"prompt\": prompt, \"image_key\": key1}, q1)\n",
    "        yield make_datapair({\"prompt\": prompt, \"image_key\": key2}, q2)\n",
    "\n",
    "\n",
    "def main(epochs: int = 1) -> None:\n",
    "    # Image-cache configuration (defaults: enabled=True, size=20); allow env overrides.\n",
    "    cache_enabled = os.environ.get(\"MARBLE_IMG_CACHE_ENABLED\", \"1\").strip() not in (\"0\", \"false\", \"False\")\n",
    "    try:\n",
    "        cache_size = int(os.environ.get(\"MARBLE_IMG_CACHE_SIZE\", \"20\"))\n",
    "    except Exception:\n",
    "        cache_size = 20\n",
    "    codec = UniversalTensorCodec()\n",
    "    ds = load_hf_streaming_dataset(\n",
    "        \"Rapidata/Imagen-4-ultra-24-7-25_t2i_human_preference\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        codec=codec,\n",
    "    )\n",
    "    # Consumed fields: prompt, image1, image2, weighted_results_image1_preference,\n",
    "    # weighted_results_image2_preference, weighted_results_image1_alignment,\n",
    "    # weighted_results_image2_alignment\n",
    "    brain = Brain(\n",
    "        2,\n",
    "        size=32,\n",
    "        bounds=((0, 31), (0, 31)),\n",
    "        formula=\"abs(n1 - n2) <= 2\",\n",
    "        store_snapshots=True,\n",
    "        snapshot_path=\".\",\n",
    "        snapshot_freq=100,\n",
    "        snapshot_keep=10,\n",
    "    )\n",
    "    # Include Brain-training plugins to adjust learning rate and step schedule\n",
    "    sa = SelfAttention(\n",
    "        routines=[\n",
    "            QualityAwareRoutine(window=8),\n",
    "            AdaptiveGradClipRoutine(),\n",
    "            FindBestNeuronTypeRoutine(),\n",
    "            ContextAwareNoiseRoutine(),\n",
    "        ]\n",
    "    )\n",
    "    wplugins = [\n",
    "        \"batchtrainer\",\n",
    "        \"qualityweightedloss\",\n",
    "        \"epsilongreedy\",\n",
    "        \"td_qlearning\",\n",
    "        \"bestlosspath\",\n",
    "        \"alternatepathscreator\",\n",
    "        \"l2_weight_penalty\",\n",
    "        \"distillation\",\n",
    "        \"wanderalongsynapseweights\",\n",
    "        \"dynamicdimensions\",\n",
    "        \"autoplugin\",\n",
    "    ]\n",
    "    neuro_cfg = {\n",
    "        \"grow_on_step_when_stuck\": True,\n",
    "        \"max_new_per_walk\": 1,\n",
    "        \"enable_prune\": True,\n",
    "        \"prune_if_outgoing_gt\": 3,\n",
    "        \"epsilongreedy_epsilon\": 0.15,\n",
    "        \"rl_epsilon\": 0.1,\n",
    "        \"rl_alpha\": 0.05,\n",
    "        \"rl_gamma\": 0.95,\n",
    "        \"l2_lambda\": 1e-4,\n",
    "        \"batch_size\": 5,\n",
    "    }\n",
    "    cache = _ImageEncodingLRUCache(max_items=cache_size, enabled=cache_enabled)\n",
    "\n",
    "    def _start_neuron(left: Dict[str, Any], br):\n",
    "        # Compose a compact input using the cached image encoding whenever available\n",
    "        key = left.get(\"image_key\")\n",
    "        enc_img = None\n",
    "        if isinstance(key, str):\n",
    "            enc_img = cache.get(key)\n",
    "        # Build a tuple combining the raw prompt and cached image tokens (or the key if missing)\n",
    "        payload = (left.get(\"prompt\"), enc_img if enc_img is not None else key)\n",
    "        enc = codec.encode(payload)\n",
    "        try:\n",
    "            idx = br.available_indices()[0]\n",
    "        except Exception:\n",
    "            idx = (0,) * int(getattr(br, \"n\", 1))\n",
    "        if idx in getattr(br, \"neurons\", {}):\n",
    "            n = br.neurons[idx]\n",
    "        else:\n",
    "            n = br.add_neuron(idx, tensor=0.0, type_name=\"autoneuron\")\n",
    "        n.receive(enc)\n",
    "        return n\n",
    "\n",
    "    for _ in range(int(epochs)):\n",
    "        pairs = _sample_pairs(ds, codec, cache)\n",
    "        run_training_with_datapairs(\n",
    "            brain,\n",
    "            pairs,\n",
    "            codec,\n",
    "            steps_per_pair=2,\n",
    "            lr=1e-3,\n",
    "            wanderer_type=\",\".join(wplugins),\n",
    "            train_type=\"warmup_decay,curriculum,qualityaware\",\n",
    "            neuro_config=neuro_cfg,\n",
    "            selfattention=sa,\n",
    "            streaming=True,\n",
    "            batch_size=5,\n",
    "            left_to_start=_start_neuron,\n",
    "        )\n",
    "    print(\"streamed quality training complete\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb164e1-b6ae-4f35-b9f1-79cecae8a9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
