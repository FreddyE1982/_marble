# YAML Manual

## Neuroplasticity Training Parameters

- aggressive_starting_neuroplasticity (bool, default: false)
  Enables an initial phase where a minimum number of neurons is added at each training step.
- add_min_new_neurons_per_step (int)
  Required when aggressive_starting_neuroplasticity is true. Minimum neurons to grow per step during the aggressive phase.
- aggressive_phase_steps (int)
  Required when aggressive_starting_neuroplasticity is true. Number of steps to keep aggressive growth before reverting to normal behaviour.

- max_flat_steps (int, default: 5)
  Maximum number of consecutive walk steps a neuron can report a ``delta`` of
  ``0.0`` before it is marked for pruning or a new synaptic connection is
  forced via plugin. Must be non-negative.

- auto_scale_targets (bool, default: false)
  When enabled, ``run_training_with_datapairs`` attaches the AutoTargetScaler
  plugin which observes early training steps and rescales targets so their
  magnitude roughly matches model outputs. This stabilizes loss when targets
  are orders of magnitude smaller than predictions.

- auto_max_steps_interval (int, default: 10)
  After every ``n`` datapairs, the Wanderer recomputes the longest path in the
  current Brain and uses that length as ``max_steps`` for subsequent walks.
  Set to ``0`` to disable automatic updates.

## Resource Allocator Settings

 - resource_allocator.max_disk_mb (int, default: 30720)
  Limits total size in MB of tensors offloaded to disk by the resource allocator.
  Must be positive. When exceeded, tensors are cleared instead of offloaded.
- resource_allocator.compress_offload (bool, default: true)
  When true, tensors moved to CPU or disk are converted to ``float16`` to save
  space. They are restored to their original dtype when reloaded.
- resource_allocator.min_gpu_tensor_mb (float, default: 0.0)
  Minimum tensor size in megabytes required before the allocator considers
  moving it to GPU. A value of ``0.0`` sends every tensor to GPU regardless of
  size.
- resource_allocator.ram_offload_threshold (float, default: 0.9)
  RAM usage ratio beyond which rarely accessed tensors are proactively
  offloaded to disk. Value must be between 0 and 1.
- resource_allocator.vram_offload_threshold (float, default: 0.9)
  VRAM usage ratio beyond which tensors are moved off the GPU to CPU or disk.
  Value must be between 0 and 1.
- resource_allocator.disk_usage_threshold (float, default: 0.95)
  Maximum allowed disk usage ratio before tensors are offloaded. Prevents disk
  exhaustion; value must be between 0 and 1.

## Reporter Settings

- reporter.tensorboard.enabled (bool, default: true)
  When set to ``true`` the central reporter mirrors every update into a live
  TensorBoard event stream. Scalars are recorded as individual time series,
  tensors and numeric sequences become histograms, and other payloads are
  emitted as text snippets so dashboards always reflect the most recent state.
- reporter.tensorboard.log_dir (str | null, default: null)
  Filesystem path passed directly to :class:`torch.utils.tensorboard.SummaryWriter`.
  Keeping the default ``null`` delegates directory creation to the writer,
  producing timestamped folders under ``runs/``. Set a custom path to aggregate
  multiple training runs in a single dashboard.
- reporter.tensorboard.flush_interval_steps (int, default: 5)
  Number of reporter updates processed before the writer flushes buffered
  events to disk. Lower values provide lower latency visualisations at the cost
  of more frequent disk synchronisation.

## AutoPlugin Settings

- autoplugin.decision_interval (int, default: 1)
  Number of walk steps between plugin selection decisions. Plugins retain their
  previous activation state on intermediate steps. Must be at least ``1``.

## Decision Controller Settings

- decision_controller.budget (float | null, default: null)
  Maximum cumulative cost allowed for plugin actions during a single decision
  step. When set to ``null`` the controller infers the limit automatically from
  observed plugin costs during a warmup phase.
- decision_controller.warmup_steps (int, default: 10)
  Number of initial steps executed without a budget limit to measure typical
  plugin costs. Must be at least 1.
- decision_controller.safety_factor (float, default: 1.2)
  Multiplier applied to the measured average cost when determining the budget
  in auto-mode. Values above ``1`` provide extra slack.
- decision_controller.recalc_interval (int, default: 100)
  Interval in steps after which the controller recalculates the budget from the
  recent average cost. A drift greater than 20% triggers an earlier update.
- decision_controller.contribution_l1 (float, default: 0.01)
  L1 penalty strength applied when training the plugin contribution regressor.
  Higher values yield sparser contribution weights.
- decision_controller.tau_threshold (float, default: 1.0)
  Minimum seconds a plugin should wait between state changes. When the actual
  interval \(\tau_t\) falls below this value, a penalty of ``tau_threshold - \tau_t``
  is added to the plugin's cost during action selection.
- decision_controller.cadence (int, default: 1)
  Number of walk steps between successive decisions of the controller. Actions
  are only reconsidered on steps that are multiples of this cadence. Must be at
  least 1.
- decision_controller.dwell_bonus (float, default: 0.1)
  Cost reduction applied for each consecutive step a plugin remains active.
  Larger values encourage persistence by making long-running plugins
  progressively cheaper under the budget constraint.
- decision_controller.lambda_lr (float, default: 0.1)
  Learning rate ``Î·`` for the adaptive Lagrange multipliers enforcing
  constraints in the policy-gradient agent. Higher values make penalties
  respond more aggressively to repeated constraint violations.
- decision_controller.dwell_threshold (int, default: 1)
  Minimum number of decision steps a plugin must remain in its current
  state before switching. Attempts to change earlier are suppressed and
  contribute to the dwell-time penalty.
- decision_controller.phase_count (int, default: 1)
  Number of discrete phases the decision controller predicts. Each phase can
  bias action logits and rescale reward weights via an internal classifier.
  Setting this above ``1`` enables phase-dependent behaviour.
- decision_controller.watch_metrics (list[str], default: [])
  Reporter item paths (``group[/subgroup]/item``) automatically queried each
  decision step. Numeric values are merged into the ``metrics`` dict supplied
  to :meth:`DecisionController.decide`.
- decision_controller.watch_variables (list[str], default: [])
  Fully qualified ``module.attr`` names of numeric variables to observe. Their
  current values are sampled every decision and merged into the ``metrics``
  dictionary.
- decision_controller.policy_mode (str, default: "policy-gradient")
  Selects the controller's action-selection algorithm. ``policy-gradient``
  trains a stochastic policy via gradient updates, while ``bayesian`` enables a
  Thompson sampler with per-plugin Gaussian posteriors.
- decision_controller.auto_cost_profile (bool, default: false)
  When ``true`` the decision controller activates the plugin cost profiler and
  refreshes per-plugin cost estimates on every decision. This allows execution
  times measured at runtime to influence future selections, disabling plugins
  whose measured cost would exceed the remaining budget. When ``false`` no
  profiling occurs and static cost hints are used.
- decision_controller.log_path (str | null, default: null)
  Filesystem path to a newline-delimited JSON log capturing every decision the
  controller makes. When set to ``null`` logging stays disabled. The file is
  opened in append mode with line buffering so observers can ``tail`` the log
  while training is running.
- decision_controller.linear_constraints.A (list[list[float]], default: [])
  Matrix ``A`` defining linear inequality constraints ``A @ a <= b`` applied
  to the binary action vector ``a``. Each row represents one constraint.
  Columns must follow the alphabetically sorted list of plugin names.
- decision_controller.linear_constraints.b (list[float], default: [])
  Upper bounds ``b`` paired with rows of ``linear_constraints.A``. When both
  lists are empty, no linear constraints are enforced.

## Reward Shaper Settings

- reward_shaper.window_size (int, default: 10)
  Number of recent performance samples kept for computing OLS trends used to
  shape actor-critic rewards. Must be at least 2.
- reward_shaper.w1..w6 (float, default: 1.0)
  Weights applied to individual reward components: latency slope (w1),
  throughput slope (w2), cost slope (w3), throughput-normalised drop (w4),
  divergence indicator (w5) and combined action penalties (w6). Adjusting these
  values emphasises or diminishes the respective component's influence on the
  total shaped reward.
- reward_shaper.M_div (float, default: 0.1)
  Threshold for activating the divergence indicator based on the
  throughput-normalised drop. When the drop exceeds this value, an additional
  penalty is applied to the reward.
