NOTE: where it says "image" below...it actually means "any data of type: image, audio, video, text"

1) Notation and goals

Image 
ğ¼
âˆˆ
ğ‘…
ğ»
Ã—
ğ‘Š
Ã—
ğ¶
IâˆˆR
HÃ—WÃ—C
 (typically 
ğ¶
=
3
C=3).

Split 
ğ¼
I into 
ğ‘
N non-overlapping patches with a linear patch embedder 
ğ‘ƒ
:
ğ‘…
ğ»
Ã—
ğ‘Š
Ã—
ğ¶
â†’
ğ‘…
ğ‘
Ã—
ğ·
P:R
HÃ—WÃ—C
â†’R
NÃ—D
.
Resulting image tokens: 
ğ‘¥
âˆˆ
ğ‘…
ğ‘
Ã—
ğ·
xâˆˆR
NÃ—D
.

Introduce 
ğ¿
L learnable latent tokens 
ğ‘§
âˆˆ
ğ‘…
ğ¿
Ã—
ğ·
zâˆˆR
LÃ—D
.

Encoder 
ğ¸
E (ViT): maps concatenated tokens to latent representations 
â„
âˆˆ
ğ‘…
ğ¿
Ã—
ğ»
ğ‘‘
hâˆˆR
LÃ—H
d
	â€‹

.

Decoder 
ğ·
D (ViT): maps 
â„
h plus learned decoder image tokens 
ğ‘’
âˆˆ
ğ‘…
ğ‘
Ã—
ğ»
ğ‘‘
eâˆˆR
NÃ—H
d
	â€‹

 back to patch embeddings 
ğ‘¥
^
âˆˆ
ğ‘…
ğ‘
Ã—
ğ·
x
^
âˆˆR
NÃ—D
, then invert patch-embedder to reconstruct image 
ğ¼
^
I
^
. 
OpenReview

2) Core forward maps

Encoder (masked input):

Draw a binary mask 
ğ‘€
âˆˆ
{
0
,
1
}
ğ‘
Mâˆˆ{0,1}
N
 with mask ratio 
ğ‘Ÿ
âˆˆ
[
0.4
,
0.6
]
râˆˆ[0.4,0.6].

Let 
ğ‘š
âˆˆ
ğ‘…
ğ·
mâˆˆR
D
 be a learnable mask token. Define masked image tokens

ğ‘¥
~
ğ‘–
â€…â€Š
=
â€…â€Š
(
1
âˆ’
ğ‘€
ğ‘–
)
â€‰
ğ‘¥
ğ‘–
â€…â€Š
+
â€…â€Š
ğ‘€
ğ‘–
â€‰
ğ‘š
,
ğ‘–
=
1
,
â€¦
,
ğ‘
.
x
~
i
	â€‹

=(1âˆ’M
i
	â€‹

)x
i
	â€‹

+M
i
	â€‹

m,i=1,â€¦,N.

Concatenate 
[
ğ‘¥
~
;
ğ‘§
]
âˆˆ
ğ‘…
(
ğ‘
+
ğ¿
)
Ã—
ğ·
[
x
~
;z]âˆˆR
(N+L)Ã—D
 and pass through the encoder:

	
â„
â€…â€Š
=
â€…â€Š
ğ¸
(
[
ğ‘¥
~
;
ğ‘§
]
)
â€…â€Š
âˆˆ
â€…â€Š
ğ‘…
ğ¿
Ã—
ğ»
ğ‘‘
.
		
(4)
h=E([
x
~
;z])âˆˆR
LÃ—H
d
	â€‹

.
(4)

(The encoder output is taken only at the latent-token positions.) 
OpenReview

Decoder (pixel path):

Concatenate learned decoder image tokens 
ğ‘’
âˆˆ
ğ‘…
ğ‘
Ã—
ğ»
ğ‘‘
eâˆˆR
NÃ—H
d
	â€‹

 with 
â„
h, then decode only from the 
ğ‘
N image-token slots:

	
ğ‘¥
^
â€…â€Š
=
â€…â€Š
ğ·
(
[
ğ‘’
;
â„
]
)
â€…â€Š
âˆˆ
â€…â€Š
ğ‘…
ğ‘
Ã—
ğ·
,
ğ¼
^
=
ğ‘ƒ
âˆ’
1
(
ğ‘¥
^
)
.
		
(5)
x
^
=D([e;h])âˆˆR
NÃ—D
,
I
^
=P
âˆ’1
(
x
^
).
(5)

Position encodings: apply 2D RoPE to image tokens 
(
ğ‘¥
,
ğ‘’
)
(x,e) and 1D absolute positions to latent tokens 
ğ‘§
z. 
OpenReview

3) Objectives

MAETok uses two decoupled objective families:

(A) Standard tokenizer (pixel-reconstruction) losses
	
ğ¿
tok
â€…â€Š
=
â€…â€Š
âˆ¥
ğ¼
âˆ’
ğ¼
^
âˆ¥
2
2
âŸ
ğ¿
rec
â€…â€Š
+
â€…â€Š
ğœ†
1
âˆ‘
â„“
âˆ¥
ğœ™
â„“
(
ğ¼
)
âˆ’
ğœ™
â„“
(
ğ¼
^
)
âˆ¥
2
2
âŸ
ğ¿
percep
â€…â€Š
+
â€…â€Š
ğœ†
2
ğ¿
adv
(
ğ¼
,
ğ¼
^
)
âŸ
e.g.,Â patch-GAN/LeCAM
.
		
(6)
L
tok
	â€‹

=
L
rec
	â€‹

âˆ¥Iâˆ’
I
^
âˆ¥
2
2
	â€‹

	â€‹

	â€‹

+Î»
1
	â€‹

L
percep
	â€‹

â„“
âˆ‘
	â€‹

âˆ¥Ï•
â„“
	â€‹

(I)âˆ’Ï•
â„“
	â€‹

(
I
^
)âˆ¥
2
2
	â€‹

	â€‹

	â€‹

+Î»
2
	â€‹

e.g.,Â patch-GAN/LeCAM
L
adv
	â€‹

(I,
I
^
)
	â€‹

	â€‹

.
(6)

Here 
ğœ™
â„“
Ï•
â„“
	â€‹

 are fixed perceptual features (e.g., VGG). In the paperâ€™s setup 
ğœ†
1
â‰ˆ
1.0
,
Â 
ğœ†
2
â‰ˆ
0.4
Î»
1
	â€‹

â‰ˆ1.0,Â Î»
2
	â€‹

â‰ˆ0.4. The adversarial term uses a discriminator with LeCAM regularization (details are implementation-level; any standard GAN loss is acceptable). 
OpenReview
+1

(B) Mask-modeling auxiliary feature-prediction losses

Attach shallow auxiliary decoders 
{
ğ·
aux
(
ğ‘—
)
}
ğ‘—
{D
aux
(j)
	â€‹

}
j
	â€‹

, each trained to predict features of masked tokens only (from 
[
ğ‘’
;
â„
]
[e;h]), e.g. HOG, DINOv2, CLIP, or BPE indices:

	
ğ‘¦
^
(
ğ‘—
)
â€…â€Š
=
â€…â€Š
ğ·
aux
(
ğ‘—
)
(
[
ğ‘’
;
â„
]
;
ğœƒ
ğ‘—
)
âˆˆ
ğ‘…
ğ‘
Ã—
ğ·
ğ‘—
,
ğ¿
mask
â€…â€Š
=
â€…â€Š
âˆ‘
ğ‘—
âˆ¥
â€‰
ğ‘€
âŠ™
(
ğ‘¦
(
ğ‘—
)
âˆ’
ğ‘¦
^
(
ğ‘—
)
)
â€‰
âˆ¥
2
2
.
		
(7â€“8)
y
^
	â€‹

(j)
=D
aux
(j)
	â€‹

([e;h];Î¸
j
	â€‹

)âˆˆR
NÃ—D
j
	â€‹

,L
mask
	â€‹

=
j
âˆ‘
	â€‹

	â€‹

MâŠ™(y
(j)
âˆ’
y
^
	â€‹

(j)
)
	â€‹

2
2
	â€‹

.
(7â€“8)

(Only masked positions contribute via 
ğ‘€
M. In practice, the 
ğ·
aux
(
ğ‘—
)
D
aux
(j)
	â€‹

 are 3-layer MLPs/decoders.) 
OpenReview
+1

Total training loss (stage 1):

Â 
ğ¿
stage1
â€…â€Š
=
â€…â€Š
ğ¿
tok
â€…â€Š
+
â€…â€Š
âˆ‘
ğ‘—
ğ›¾
ğ‘—
â€‰
ğ¿
mask
(
ğ‘—
)
Â 
Â L
stage1
	â€‹

=L
tok
	â€‹

+
j
âˆ‘
	â€‹

Î³
j
	â€‹

L
mask
(j)
	â€‹

Â 
	â€‹


(Use equal or tuned 
ğ›¾
ğ‘—
Î³
j
	â€‹

.)

4) Two-stage training schedule

Stage-1 (MAE training):

Mask ratio 
ğ‘Ÿ
âˆˆ
[
0.4
,
0.6
]
râˆˆ[0.4,0.6].

Optimize 
ğ¿
stage1
L
stage1
	â€‹

 jointly for encoder 
ğ¸
E, pixel decoder 
ğ·
D, and auxiliary decoders 
ğ·
aux
(
ğ‘—
)
D
aux
(j)
	â€‹

.

Goal: make the latent space (in 
â„
h) discriminative while preserving usable pixel reconstruction.

Stage-2 (pixel-decoder fine-tuning):

Freeze 
ğ¸
E (to preserve the learned latent geometry).

Linearly anneal mask ratio 
ğ‘Ÿ
:
0.6
â†’
0
r:0.6â†’0 (e.g., over 50k iterations).

Optimize only 
ğ·
D (and optionally 
ğ‘ƒ
âˆ’
1
P
âˆ’1
) with 
ğ¿
tok
L
tok
	â€‹

 (auxiliary heads can be dropped).

Goal: recover high-fidelity pixels without wrecking the encoderâ€™s discriminative latent structure. 
OpenReview

5) Token counts and shapes (practical defaults)

Latent tokens: 
ğ¿
=
128
L=128 (what you pass to the diffusion model).

Mask ratio: 
ğ‘Ÿ
âˆˆ
[
0.4
,
0.6
]
râˆˆ[0.4,0.6] in Stage-1; anneal to 
0
0 in Stage-2.

Transformer details: ViT encoder/decoder; 1D tokenization (â€œlearnable latent tokensâ€ + image tokens); use RoPE 2D for image tokens and 1D absolute for latent tokens. (Hidden widths, heads, layers are architectural choices; the paper used ViT-Base for the tokenizer.) 
OpenReview
+1

6) What the diffusion model sees

At inference/conditioning time (no masking):

Encode: 
â„
=
ğ¸
(
[
ğ‘¥
;
ğ‘§
]
)
h=E([x;z]) with 
ğ‘€
â‰¡
0
Mâ‰¡0.

Provide the 
ğ¿
Ã—
ğ»
ğ‘‘
LÃ—H
d
	â€‹

 latent sequence 
â„
h to a 1D diffusion transformer (e.g., SiT/LightningDiT) as the working latent.

Decode: after diffusion sampling returns a latent sequence 
â„
~
h
~
, produce the image with 
ğ‘¥
^
=
ğ·
(
[
ğ‘’
;
â„
~
]
)
x
^
=D([e;
h
~
]) and 
ğ¼
^
=
ğ‘ƒ
âˆ’
1
(
ğ‘¥
^
)
I
^
=P
âˆ’1
(
x
^
).
The whole point: a compact (e.g., 128-token) latent with a well-structured distribution (empirically: fewer GMM modes) that accelerates and stabilizes diffusion training/sampling. 
OpenReview
+1

7) Why the latent looks â€œniceâ€ (theory shorthand)

If the latent distribution is approximable by a 
ğ¾
K-component GMM, the paper shows (via existing DDPM bounds) that to reach a target KL error the required sample size scales like 
ğ‘‚
(
ğ¾
4
)
O(K
4
). Fewer effective modes 
ğ¾
â‡’
Kâ‡’ easier diffusion learning; mask-modeling + auxiliary feature prediction empirically reduces effective 
ğ¾
K vs. plain AE/VAE. 
OpenReview

8) Minimal checklist to reproduce MAETok

Build 
ğ‘ƒ
,
ğ‘ƒ
âˆ’
1
P,P
âˆ’1
, ViT-
ğ¸
E, ViT-
ğ·
D, learnable tokens 
ğ‘§
,
ğ‘š
,
ğ‘’
z,m,e.

Stage-1: sample mask 
ğ‘€
M, forward 
ğ‘¥
~
â†’
â„
x
~
â†’h, decode 
ğ¼
^
I
^
, compute 
ğ¿
tok
L
tok
	â€‹

; compute auxiliary targets 
ğ‘¦
(
ğ‘—
)
y
(j)
 and 
ğ¿
mask
L
mask
	â€‹

 on masked positions only; update 
ğ¸
,
ğ·
,
{
ğ·
aux
(
ğ‘—
)
}
E,D,{D
aux
(j)
	â€‹

}.

Stage-2: freeze 
ğ¸
E, anneal 
ğ‘Ÿ
â†’
0
râ†’0, optimize 
ğ·
D on 
ğ¿
tok
L
tok
	â€‹

 to sharpen pixels.

Use 
ğ¿
=
128
L=128 latent tokens for downstream diffusion. 
OpenReview
+1