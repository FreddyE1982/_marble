resource_allocator:
  max_disk_mb: 30720  # maximum MB for disk offload (30 GB)
  compress_offload: true  # store offloaded tensors in float16
  min_gpu_tensor_mb: 0.0  # send all tensors to GPU regardless of size
  ram_offload_threshold: 0.9  # offload tensors to disk when RAM usage exceeds this ratio
  vram_offload_threshold: 0.9  # move tensors off GPU when VRAM usage exceeds this ratio
  disk_usage_threshold: 0.95  # stop offloading when disk usage exceeds this ratio
autoplugin:
  decision_interval: 1  # steps between plugin selector decisions
decision_controller:
  budget: 10.0  # maximum total cost for selected actions
  contribution_l1: 0.01  # L1 penalty for contribution regressor
  tau_threshold: 1.0  # minimum seconds between state changes before penalty
  cadence: 1  # walk steps between controller decisions
  dwell_bonus: 0.1  # cost reduction per consecutive active step
  lambda_lr: 0.1  # learning rate for constraint multipliers
  dwell_threshold: 1  # minimum steps before a plugin may change state
  phase_count: 1  # number of discrete phases for gating
  watch_metrics: []  # reporter paths to monitor automatically
  watch_variables: []  # module.variable paths to observe
  policy_mode: policy-gradient  # 'policy-gradient' or 'bayesian'
  linear_constraints:
    A: []  # constraint matrix for actions
    b: []  # upper bounds for linear constraints
max_flat_steps: 5  # consecutive zero-delta steps before pruning/connection
auto_scale_targets: false  # scale tiny targets to match output magnitude
auto_max_steps_interval: 10  # recompute wanderer max_steps every N datapairs
reward_shaper:
  window_size: 10  # number of recent steps for OLS
  w1: 1.0  # weight for latency slope
  w2: 1.0  # weight for throughput slope
  w3: 1.0  # weight for cost slope
  w4: 1.0  # weight for throughput-normalized drop
  w5: 1.0  # weight for divergence indicator
  w6: 1.0  # weight for action penalties
  M_div: 0.1  # divergence threshold for throughput drop
